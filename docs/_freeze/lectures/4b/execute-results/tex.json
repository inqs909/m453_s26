{
  "hash": "6e68698a6adbaca5b9a1ff80df3c491c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Joint Distribution Functions\"\nformat:\n  beamer:\n    theme: gotham\n    colortheme: dolphin\n    fonttheme: professionalfonts\n    fontsize: 14pt\nheader-includes:\n  - \\setbeamertemplate{footline}{\n      \\hfill\n      \\usebeamerfont{page number in head/foot}\n      \\insertframenumber{} / \\inserttotalframenumber\n      \\kern1em\\vskip2pt\n    }\n  - \\gothamset{sectionframe default=off}\n  - \\usepackage{amsmath}\n  - \\usepackage{amssymb}\n\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n\neditor: source\n---\n\n\n\n\n# Conditional Distributions\n\n## Conditional Distributions\n\nA conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable.\n\n## Discrete Conditional Distributions\n\nLet $X$ and $Y$ be 2 discrete random variables, with a joint distribution function of\n\n$$\np_{X,Y}(x, y) = P(X = x, Y = y)\n$$\n\nThe conditional distribution of $X|Y=y$ is defined as\n\n$$\np_{X|Y = y}(x) = \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\n$$\n\n## Continuous Conditional Distributions\n\nLet $X$ and $Y$ be 2 continuous random variables, with a joint density function of $f_{X,Y}(x,y)$. The conditional distribution of $X|Y=y$ is defined as\n\n$$\nf_{X|Y=y}(x)= \\frac{f_{X,Y}(x,y)}{f_{Y}(y)}\n$$\n\n## Bivariae Normal Conditional Distribution\n\n\n# Multivariate Normal\n\n## Multivariate Normal\n\nThe normal distribution function can be extended to from univariate (n=1), bivariate (n=2), to a k-dimensional normal distribution function:\n\n$$\n\\boldsymbol X \\sim N_k(\\boldsymbol \\mu, \\boldsymbol \\Sigma)\n$$\n\n\n## Multivariate Normal Parameters\n\n```{=latex}\n{\\small\n\\begin{equation*}\n\\boldsymbol X = \\left(\n  \\begin{array}{c}\n  X_1 \\\\\n  \\vdots \\\\\n  X_k\n  \\end{array}\n\\right)\\\n\\boldsymbol \\mu = \\left(\n  \\begin{array}{c}\n  \\mu_1 \\\\\n  \\vdots \\\\\n  \\mu_k\n  \\end{array}\n\\right)\\ \n\\boldsymbol \\Sigma = \\left(\n  \\begin{array}{cccc}\n  \\sigma_1^2 & \\sigma_{21} & \\cdots & \\sigma_{k1} \\\\\n  \\sigma_{21} & \\sigma_2^2 & \\cdots & \\vdots \\\\\n  \\vdots  & \\cdots & \\ddots & \\vdots \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_k^2\n  \\end{array}\n\\right)\n\\end{equation*}\n}\n```\n\n## Multivariate Normal PDF\n\n$$\n{\\small\nf_{\\boldsymbol X} (\\boldsymbol x) = \n\\det(2\\pi\\boldsymbol\\Sigma)^{-1/2}\\exp\\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol\\mu)^\\mathrm{T}\\boldsymbol\\Sigma^{-1}(\\boldsymbol x-\\boldsymbol\\mu)\\right\\}\n}\n$$\n\n## Marginal Normal Distribution\n\n```{=latex}\n{\\small\n\\begin{equation*}\nX = \\left(\n  \\begin{array}{c}\n  \\mathcal X_1 \\\\\n  \\mathcal X_2\n  \\end{array}\n\\right)\\\n\\mathcal X_1 = \\left(\n  \\begin{array}{c}\n  X_1 \\\\\n  \\vdots \\\\\n  X_j\n  \\end{array}\n\\right)\\ \n\\mathcal X_2 = \\left(\n  \\begin{array}{c}\n  X_{j+1} \\\\\n  \\vdots \\\\\n  X_k\n  \\end{array}\n\\right)\\ \n\\end{equation*}\n}\n```\n\n## Marginal Normal Distribution\n\n```{=latex}\n{\\small\n\\begin{equation*}\n\\mu = \\left(\n  \\begin{array}{c}\n  \\varpi_1 \\\\\n  \\varpi_2\n  \\end{array}\n\\right)\\\n\\varpi_1 = \\left(\n  \\begin{array}{c}\n  \\mu_1 \\\\\n  \\vdots \\\\\n  \\mu_j\n  \\end{array}\n\\right)\\ \n\\varpi_2 = \\left(\n  \\begin{array}{c}\n  \\mu_{j+1} \\\\\n  \\vdots \\\\\n  \\mu_k\n  \\end{array}\n\\right)\\ \n\\end{equation*}\n}\n```\n\n## Marginal Normal Distribution\n\n```{=latex}\n\\begin{equation*}\n\\Sigma = \\left(\n  \\begin{array}{cc}\n  \\varSigma_{11} & \\varSigma_{21} \\\\\n  \\varSigma_{12} & \\varSigma_{22}\n  \\end{array}\n\\right)\n\\end{equation*}\n```\n## Marginal Normal Distribution\n\n$$\n\\mathcal X_1 \\sim N_j(\\varpi_1, \\varSigma_{11})\n$$\n\n$$\n\\mathcal X_2 \\sim N_{k-j}(\\varpi_2, \\varSigma_{22})\n$$\n\n## Conditional Normal Distribution\n\n$$\n\\mathcal X_2|\\mathcal X_1 \\sim N_j(\\varpi_2 + \\varSigma_{21}\\varSigma_{11}^{-1}(\\mathcal X_1 - \\varpi_1), \\varSigma_{22}-\\varSigma_{21}\\varSigma_{11}^{-1}\\varSigma_{12})\n$$\n\n\n\n\n# Independence\n\n## Independent Random Variables\n\nRandom variables are considered independent of each other if the probability of one variable does not affect the probability of another variable.\n\n## Discrete Independent Random Variables\n\nLet $X$ and $Y$ be 2 discrete random variables, with a joint density function of $p_{X,Y}(x,y)$. $X$ is independent of $Y$ if and only if\n\n$$\np_{X,Y}(x,y) = p_{X}(x)p_{Y}(y)\n$$\n\n## Continuous Independent Random Variables\n\nLet $X$ and $Y$ be 2 continuous random variables, with a joint density function of $f_{X,Y}(x,y)$. $X$ is independent of $Y$ if and only if\n\n$$\nf_{X,Y}(x,y) = f_{X}(x)f_{Y}(y)\n$$\n\n## Matrix Algebra\n\n$$\nA = \\left(\\begin{array}{cc}\na_1 & 0\\\\\n0 & a_2\n\\end{array}\\right)\n$$\n\n$$\n\\det(A) = a_1a_2\n$$\n\n$$\nA^{-1}=\\left(\\begin{array}{cc}\n1/a_1 & 0 \\\\\n0 & 1/a_2\n\\end{array}\\right)\n$$\n\n## Example\n\n$$\n\\left(\\begin{array}{c}\nX\\\\\nY\n\\end{array}\\right)\\sim N_2 \\left\\{\n\\left(\\begin{array}{c}\n\\mu_x\\\\\n\\mu_y\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\sigma_x^2 & 0\\\\\n0 & \\sigma_y^2\n\\end{array}\\right)\n\\right\\}\n$$\n\nShow that $X\\perp Y$ (**this will be on the upcoming test!**).\n\n\n# Covariance\n\n## Covariance\n\nLet $X$ and $Y$ be 2 random variables with mean $E(X)=\\mu_x$ and $E(Y)=\\mu_y$, respectively. The covariance of $X$ and $Y$ is defined as\n\n\n$$\nCov(X,Y) = E\\{(X-\\mu_x)(Y-\\mu_y)\\}\n$$\n\n$$\nCov(X,Y) = E(XY)-\\mu_x\\mu_y\n$$\n\n\n## Covariance\n\nIf $X$ and $Y$ are independent random variables, then\n\n$$\nCov(X,Y)=0\n$$\n\n## Correlation\n\nThe correlation of $X$ and $Y$ is defined as\n\n$$\n\\rho = Cor(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\n$$\n\n\n# Expectations\n\n## Expectations\n\nLet $\\boldsymbol X = (X_1, X_2, \\ldots,X_n)^\\mathrm{T}$ be a set of random variables, the expectation of a function $g(\\boldsymbol X)$ is defined as\n\n$$\nE\\{g(\\boldsymbol X)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(\\boldsymbol X)p(\\boldsymbol x, \\boldsymbol \\theta)\n$$\n\nor\n\n$$\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol x, \\boldsymbol \\theta)dx_n \\cdots dx_1\n$$\n\n\n## Expected Value and Variance of Linear Functions\n\nLet $X_1,\\ldots,X_n$ and $Y_1,\\ldots,Y_m$ be random variables with $E(X_i)=\\mu_i$ and $E(Y_j)=\\tau_j$. Furthermore, let $U = \\sum^n_{i=1}a_iX_i$ and $V=\\sum^m_{j=1}b_jY_j$ where $\\{a_i\\}^n_{i=1}$ and $\\{b_j\\}_{j=1}^m$ are constants. We have the following properties:\n\n-   $E(U)=\\sum_{i=1}^na_i\\mu_i$\n\n-   $Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i<j}{\\sum\\sum}a_ia_jCov(X_i,X_j)$\n\n-   $Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)$\n\n## Conditional Expectations\n\nLet $X_1$ and $X_2$ be two random variables, the conditional expectation of $g(X_1)$, given $X_2=x_2$, is defined as\n\n$$\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n$$\n\nor\n\n$$\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n$$\n\n## Conditional Expectations\n\nFurthermore,\n\n$$\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n$$\n\nand\n\n$$\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n$$\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}