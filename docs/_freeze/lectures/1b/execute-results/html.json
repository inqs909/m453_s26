{
  "hash": "0ca81dfc0abb3e1ddf4a02fc644d959d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Review:\"\nsubtitle: \"Random Variables and Distribution Functions\"\nformat:\n  revealjs:\n    include-in-header: \"math_commands.html\"\n    width: 1200\n    scrollable: true\n    sc-sb-title: true\n    footer: m453.inqs.info/lectures/1a\n    theme: [default, styles.scss]\n    navigation-mode: vertical\n    controls-layout: edges\n    controls-tutorial: true\n    slide-number: true\n    pointer:\n      pointerSize: 32\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  - verticator\n  \nfilters: \n  - reveal-header\n  - code-fullscreen\n  - reveal-auto-agenda\n\neditor: source\n---\n\n# Review Random Variables\n\n## Random Variables\n\nA random variable is function that maps the sample space to real value.\n\n# Discrete Random Variables\n\n## Discrete Random Variables\n\nA random variable is considered to be discrete if it can only map to a finite or countably infinite number of distinct values.\n\n## PMF\n\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as $P(Y=y)$ for all values of $y$.\n\n## CDF\n\nThe cumulative distribution function provides the $P(Y\\leq y)$ for a random variable $Y$.\n\n## Expected Value\n\nThe *expected value* is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of Y is\n\n$$\nE(Y)=\\sum_y yP(y)\n$$\n\n## Variance\n\nThe *variance* is the expected squared difference between the random variable and expected value.\n\n$$\nVar(Y)=\\sum_y\\{y-E(Y)\\}^2P(y)\n$$\n\n$$\nVar(Y) = E(X^2) - E(X)^2\n$$\n\n## Known Distributions\n\n| Distribution      | Parameter(s)      | PMF $P(Y=y)$                          |\n|-------------------|-------------------|---------------------------------------|\n| Bernoulli         | $p$               | $p$                                   |\n| Binomial          | $n$ and $p$       | $(^n_y)p^y(1-p)^{n-p}$                |\n| Geometric         | $p$               | $(1-p)^{y-1}p$                        |\n| Negative Binomial | $r$ and $p$       | $(^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}$    |\n| Hypergeometric    | $N$, $n$, and $r$ | $\\frac{(^r_y)(^{N-r}_{n-y})}{(^N_n)}$ |\n| Poisson           | $\\lambda$         | $\\frac{\\lambda^y}{y!} e^{-\\lambda}$   |\n\n# Binomial Distribution\n\n## Binomial Distribution\n\nAn experiment is said to follow a binomial distribution if\n\n1.  Fixed $n$\n2.  Each trial has 2 outcomes\n3.  The probability of success is a constant $p$\n4.  The trials are independent of each\n\n::: fragment\n$P(X=x)=(^n_x)p^x(1-p)^{n-x}$\n:::\n\n## Expected Value of a Binomial Distribution\n\n## Continued\n\n# Poisson Distribution\n\n## Poisson Distribution\n\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period.\n\n::: fragment\n$P(X=x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}$\n:::\n\n## Expected Value of a Poisson Distribution\n\n# Continuous Random Variables\n\n## Continuous Random Variables\n\nA random variable $X$ is considered continuous if the $P(X=x)$ does not exist.\n\n## CDF\n\nThe cumulative distribution function of $X$ provides the $P(X\\leq x)$, denoted by $F(x)$, for the domain of $X$.\n\nProperties of the CDF of $X$:\n\n1.  $F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0$\n2.  $F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1$\n3.  $F(x)$ is a nondecreaseing function\n\n## PDF\n\nThe probability density function of the random variable $X$ is given by\n\n$$\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n$$\n\nwherever the derivative exists.\n\nProperties of pdfs:\n\n1.  $f(x)\\geq 0$\n2.  $\\int^\\infty_{-\\infty}f(x)dx=1$\n3.  $P(a\\leq X\\leq b) = P(a<X<b)=\\int^b_af(x)dx$\n\n## Expected Value\n\nThe expected value for a continuous distribution is defined as\n\n$$\nE(X)=\\int x f(x)dx\n$$\n\nThe expectation of a function $g(X)$ is defined as\n\n$$\nE\\{g(X)\\}=\\int g(x)f(x)dx\n$$\n\n## Expected Value Properties\n\n1.  $E(c)=c$, where $c$ is constant\n2.  $E\\{cg(X)\\}=cE\\{g(X)\\}$\n3.  $E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}$\n\n## Variance\n\nThe variance of continuous variable is defined as\n\n$$\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx \n$$\n\n# Uniform Distribution\n\n## Uniform Distribution\n\nA random variable is said to follow uniform distribution if the density function is constant between two parameters.\n\n::: fragment\n$$\nf(x) = \\left\\{\\begin{array}{cc}\n \\frac{1}{b-a} & a \\leq x \\leq b\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n$$\n:::\n\n## Expected Value\n\n# Normal Distribution\n\n## Normal Distribution\n\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function.\n\n::: fragment\n$$\nf(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n$$\n:::\n\n## Expected Value\n\n## Continued\n",
    "supporting": [
      "1b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}