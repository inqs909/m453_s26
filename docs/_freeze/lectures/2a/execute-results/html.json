{
  "hash": "fef9d2c6a94b4b5dc2b032b871b05a26",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Review:\"\nsubtitle: \"More Probability Theory\"\nformat:\n  revealjs:\n    include-in-header: \"math_commands.html\"\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      theme: whiteboard\n      chalk-width: 4\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n    \nrevealjs-plugins:\n  - pointer\n  \nfilters: \n  - reveal-auto-agenda\n\neditor: source\n---\n\n# Continuous Random Variables\n\n## Continuous Random Variables\n\nA random variable $X$ is considered continuous if the $P(X=x)$ does not exist.\n\n## CDF\n\nThe cumulative distribution function of $X$ provides the $P(X\\leq x)$, denoted by $F(x)$, for the domain of $X$.\n\nProperties of the CDF of $X$:\n\n1.  $F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0$\n2.  $F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1$\n3.  $F(x)$ is a nondecreaseing function\n\n## PDF\n\nThe probability density function of the random variable $X$ is given by\n\n$$\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n$$\n\nwherever the derivative exists.\n\nProperties of pdfs:\n\n1.  $f(x)\\geq 0$\n2.  $\\int^\\infty_{-\\infty}f(x)dx=1$\n3.  $P(a\\leq X\\leq b) = P(a<X<b)=\\int^b_af(x)dx$\n\n## Expected Value\n\nThe expected value for a continuous distribution is defined as\n\n$$\nE(X)=\\int x f(x)dx\n$$\n\nThe expectation of a function $g(X)$ is defined as\n\n$$\nE\\{g(X)\\}=\\int g(x)f(x)dx\n$$\n\n## Expected Value Properties\n\n1.  $E(c)=c$, where $c$ is constant\n2.  $E\\{cg(X)\\}=cE\\{g(X)\\}$\n3.  $E\\{g_1(X)+g_2(X)+\\cdots+g_n(X)\\}=E\\{g_1(X)\\}+E\\{g_2(X)\\}+\\cdots+E\\{g_n(X)\\}$\n\n## Variance\n\nThe variance of continuous variable is defined as\n\n$$\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx \n$$\n\n# Uniform Distribution\n\n## Uniform Distribution\n\nA random variable is said to follow uniform distribution if the density function is constant between two parameters.\n\n::: fragment\n$$\nf(x) = \\left\\{\\begin{array}{cc}\n \\frac{1}{b-a} & a \\leq x \\leq b\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n$$\n:::\n\n## Expected Value\n\n# Normal Distribution\n\n## Normal Distribution\n\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function.\n\n::: fragment\n$$\nf(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n$$\n:::\n\n## Expected Value\n\n\n# Moment Generating Functions\n\n## Moments\n\nThe $k$th moment is defined as the expectation of the random variable, raised to the $k$th power, defined as $E(X^k)$.\n\n## Moment Generating Functions\n\nThe moment generating functions is used to obtain the $k$th moment. The mgf is defined as\n\n$$\nm(t) = E(e^{tX})\n$$\n\nThe $k$th moment can be obtained by taking the $k$th derivative of the mgf, with respect to $t$, and setting $t$ equal to 0:\n\n$$\nE(X^k)=\\frac{d^km(t)}{dt}\\Bigg|_{t=0}\n$$\n\n# Characteristic Functions\n\n## Characteristic Functions\n\n$$\n\\phi(t) = E\\left(e^{itX}\\right) = E\\left\\{\\cos(tX)\\right\\}  + iE\\left\\{\\sin(tX)\\right\\}\n$$\n\n# Poisson Distribution\n\n## MGF\n\n## Expected Value\n\n## Variance\n\n## Variance\n\n# Binomial Distribution\n\n## MGF\n\n# Uniform Distribution\n\n## MGF\n\n# Normal Distribution\n\n## MGF\n\n# MGF Properties\n\n## Linearity\n\nLet $X$ follow a distribution $f$, with the an MGF $M_X(t)$, the MGF of $Y=aX+b$ is given as\n\n$$\nM_Y(t) = e^{tb}M_X(at)\n$$\n\n## Derivation\n\n## Linearity\n\nLet $X$ and $Y$ be two random variables with MGFs $M_X(t)$ and $M_Y(t)$, respectively, and are independent. The MGF of $U=X-Y$\n\n$$\nM_U(t) = M_X(t)M_Y(-t)\n$$\n\n## Derivation\n\n## Uniqueness\n\nLet $X$ and $Y$ have the following distributions $F_X(x)$ and $F_Y(y)$ and MGFs $M_X(t)$ and $M_Y(t)$, respectively. $X$ and $Y$ have the same distribution $F_X(x)=F_Y(y)$ if and only if $M_X(t)=M_Y(t)$.\n\n## Uniqueness\n\nLet $X_1,\\cdots, X_n$ be independent random variables, where $X_i\\sim N(\\mu_i, \\sigma^2_i)$, with $M_{X_i}(t)=\\exp\\{\\mu_i t+\\sigma^2_it^2/2\\}$ for $i=1,\\cdots, n$. Find the MGF of $Y=a_1X_1+\\cdots+a_nX_n$, where $a_1, \\cdots, a_n$ are constants.\n",
    "supporting": [
      "2a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}