---
title: "Sampling Distrbutions"
format:
  beamer:
    theme: gotham
    colortheme: dolphin
    fonttheme: professionalfonts
    fontsize: 14pt
header-includes:
  - \setbeamertemplate{footline}{
      \hfill
      \usebeamerfont{page number in head/foot}
      \insertframenumber{} / \inserttotalframenumber
      \kern1em\vskip2pt
    }
  - \gothamset{sectionframe default=off}
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 

editor: source
---

```{r}
#| include: false

knitr::opts_chunk$set(echo = T)

```



# Expectations

## Expectations

Let $\boldsymbol X = (X_1, X_2, \ldots,X_n)^\mathrm{T}$ be a set of random variables, the expectation of a function $g(\boldsymbol X)$ is defined as

```{=latex}
{\small
\begin{equation*}
E\{g(\boldsymbol X)\} = \sum_{x_1\in X_1}\cdots\sum_{x_n\in X_n}g(\boldsymbol X)p(\boldsymbol x, \boldsymbol \theta)
\end{equation*}
}
```

or

```{=latex}
{\small
\begin{equation*}
E\{g(\boldsymbol X)\} = \int_{x_1\in X_1}\cdots\int_{x_n\in X_n}g(\boldsymbol X)f(\boldsymbol x, \boldsymbol \theta)dx_n \cdots dx_1
\end{equation*}
}
```


## Expected Value and Variance of Linear Functions

Let $X_1,\ldots,X_n$ and $Y_1,\ldots,Y_m$ be random variables with $E(X_i)=\mu_i$ and $E(Y_j)=\tau_j$. Furthermore, let $U = \sum^n_{i=1}a_iX_i$ and $V=\sum^m_{j=1}b_jY_j$ where $\{a_i\}^n_{i=1}$ and $\{b_j\}_{j=1}^m$ are constants. We have the following properties:

-   $E(U)=\sum_{i=1}^na_i\mu_i$

-   $Var(U)=\sum^n_{i=1}a_i^2Var(X_i)+2\underset{i<j}{\sum\sum}a_ia_jCov(X_i,X_j)$

-   $Cov(U,V)=\sum^n_{i=1}\sum^m_{j=1}Cov(X_i,Y_j)$

## Conditional Expectations

Let $X_1$ and $X_2$ be two random variables, the conditional expectation of $g(X_1)$, given $X_2=x_2$, is defined as

$$
E\{g(X_1)|X_2=x_2\}=\sum_{x_1}g(x_1)p(x_1|x_2)
$$

or

$$
E\{g(X_1)|X_2=x_2\}=\int_{x_1}g(x_1)f(x_1|x_2)dx_1.
$$

## Conditional Expectations

Furthermore,

```{=latex}
{\small
\begin{equation*}
E(X_1)=E_{X_2}\{E_{X_1|X_2}(X_1|X_2)\}
\end{equation*}
}
```
and

```{=latex}
{\footnotesize
\begin{equation*}
Var(X_1) = E_{X_2}\{Var_{X_1|X_2}(X_1|X_2)\} + Var_{X_2}\{E_{X_1|X_2}(X_1|X_2)\}
\end{equation*}
}
```


# Statistics

## Sample

When collecting data to construct a sample, the sample is a collection of random variables. Therefore, the sample can be subjected to probability properties.

## iid Random Variables

A sample of random variables are said to be iid if they are identical and independentally distributed.

For example, $X$ and $Y$ are iid, if $X$ and $Y$ has the same distribution $f(\theta)$ and $X \perp  Y$

## Statistics

A statistic is a transformation of the the sample data. Before data is calculated, a statistic from a sample can take any value. Therefore, a statistic must be a random variable.


## Mean

Let $X_1, X_2, \ldots, X_n$ be a random sample that is *iid*.
The mean is defined as:

$$
\bar X = \frac{1}{n}\sum^n_{i=1}X_i
$$

## Variance

Let $X_1, X_2, \ldots, X_n$ be a random sample that is *iid*.
The variance is defined as:

```{=latex}
{\small
\begin{equation*}
s^2 = \frac{1}{n-1}\sum^n_{i=1} (X_i - \bar X)^2 = \frac{1}{n-1}\sum^n_{i=1}X_i - n \bar{X}^2
\end{equation*}
}
```



# Sampling Distributions

## Sampling Distributions

A sampling distribution is the distribution of a statistic. Many known statistics have a known distribution.

## $\bar X$

Let $X_1, X_2, \ldots, X_n\overset{iid}{\sim}N(\mu,\sigma^2)$ , show that $\bar X \sim N(\mu,\sigma^2/n)$. 

## Sum of $\chi^2_1$

Let $Z_1^2, \ldots, Z_n^2$ be a iid $\chi^2_1$. Find $Y = \sum^n_{i=1} Z_i^2$ 

## $s^2$

## t-distribution

Let $Z\sim N(0,1)$, $W\sim \chi^2_\nu$, $Z\perp W$; therefore:

$$
T=\frac{Z}{\sqrt{W/\nu}} \sim t_\nu
$$

## F-distribution

Let $W_1\sim\chi^2_{\nu_1}$ $W_2\sim\chi^2_{\nu_2}$, and $W_1\perp W_2$; therefore:

$$
F = \frac{W_1/\nu_1}{W_2/\nu_2}\sim F_{\nu_1,\nu_2}
$$

# Order Statistics

##  Order Statistics

Order statistics are a fundamental concept in statistics and probability, dealing with the properties of sorted random variables. They provide insights into the distribution and behavior of sample data, such as minimum, maximum, and quantiles. Understanding order statistics is crucial in various fields such as risk management, quality control, and data analysis.

## Order Statistics

  Let $X_1, X_2, \ldots, X_n$ be a sample of $n$ independent and identically distributed (i.i.d.) random variables with a common probability density function $f(x)$. The order statistics are the sorted values of this sample, denoted as:
  
$$
X_{(1)} \leq X_{(2)} \leq \cdots \leq X_{(n)}
$$
  
  Here, $X_{(1)}$ is the minimum, and $X_{(n)}$ is the maximum of the sample.

## Order Statistics

  - $X_{(k)}$: The $k$-th order statistic, representing the $k$-th smallest value in the sample.
  - $X_{(1)}, X_{(n)}$: The minimum and maximum of the sample, respectively.

## Distribution of Order Statistic

The distribution of the $k$-th order statistic $X_{(k)}$ can be derived using combinatorial arguments. Its PDF is given by:
$$
f_{X_{(k)}}(x) = \frac{n!}{(k-1)!(n-k)!} [F(x)]^{k-1} [1 - F(x)]^{n-k} f(x)
$$

  This formula shows how the distribution of $X_{(k)}$ depends on the underlying distribution of the sample and its position $k$.



# Central Limit Theorem

## Central Limit Theorem

Let $X_1, X_2, \ldots, X_n$ be identical and independent distributed random variables with $E(X_i)=\mu$ and $Var(X_i) = \sigmaÂ²$. We define

$$
Y_n = \sqrt n \left(\frac{\bar X-\mu}{\sigma}\right) \mathrm{ where }\ \bar X = x\frac{1}{n}\sum^n_{i=1}X_i.
$$

Then, the distribution of the function $Y_n$ converges to a standard normal distribution function as $n\rightarrow \infty$.

## Central Limit Theorem

$$
\bar X \sim N\left(\mu, \frac{\sigma^2}{n}\right)
$$

## Central Limit Theorem Proof


## Example

Let $X_1, \ldots, X_n \overset{iid}{\sim} \chi^2_p$, the MGF is $M(t)=(1-2t)^{-p/2}$. Find the distribution of $\bar X$ as $n \rightarrow \infty$.
