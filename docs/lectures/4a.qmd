---
title: "Joint Distribution Functions"
format:
  beamer:
    theme: gotham
    colortheme: dolphin
    fonttheme: professionalfonts
    fontsize: 14pt
header-includes:
  - \setbeamertemplate{footline}{
      \hfill
      \usebeamerfont{page number in head/foot}
      \insertframenumber{} / \inserttotalframenumber
      \kern1em\vskip2pt
    }
  - \gothamset{sectionframe default=off}
knitr:
  opts_chunk: 
    echo: true
    eval: true
    message: false
    warnings: false
    comment: "#>" 

editor: source
---

```{r}
#| include: false

knitr::opts_chunk$set(
  echo = F,
  message = F
  )

```

# Joint Distributions

## Partial Derivatives

For a function $f(x,y)$, the partial derivative with respect to $x$ is taken by differentiating $f(x,y)$ with respect to $x$ while treating $y$ as a constant. For example:

$f(x,y) = x^2 + \ln(y)$

## Multiple Integration

Multiple integration is when you integrate a multivariate function by multiple variables. This is done by integrating the function by an individual variable at a time. For example:

$f(x,y)=x^2 + y^2$ which can be integrated as:

## Joint Distributions

A joint distribution is a process where more than one random variable is generated; for example, collecting biomedical data, such as multiple biomarkers, are considered to follow a joint distribution. In mathematical terms, instead of dealing with a random variable, we are dealing with a random vector. Observing a particular random vector will have a probability attached to it.

## Bivariate Discrete Distributions

Let $X$ and $Y$ be 2 discrete random variables, the joint distribution function of $(X, Y)$ is defined as

$$
p_{X,Y}(x, y) = P(X = x, Y = y)
$$

The properties of a bivariate discrete distribution are

-   $p_{X,Y}(x, y) \ge 0$ for all $x,\ y$

-   $\sum_{x}\sum_{y}p_{X,Y}(x,y)=1$

## Bivariate Continuous Distribution

Let $X$ and $Y$ be 2 continuous random variables, the joint distribution function of $(X, Y)$ is defined as

$$
F_{X, Y}(x, y) = P(X\le x, Y \le y).
$$

The properties of a bivariate continuous distribution are

-   $f_{X,Y}(x,y)=\frac{\partial^2F_{X, Y}}{\partial x\partial y}$

-   $f_{X,Y}(x, y)\ge 0$

-   $\int_{x}\int_{y}f_{X,Y}(x,y)dy dx=1$

## Example

$$
f(x,y) \left\{\begin{array}{cc}
3x & 0\le y\le x\le 1\\
0 & \mathrm{otherwise}
\end{array}\right.
$$

Find $P(0\le X\le 0.5,0.25\le Y)$


# Bivariate Normal Distribution

## Bivariate Normal Distribution

$$
\left(
\begin{array}{c}
X \\
Y
\end{array}
\right) \sim
N\left[
\left(
\begin{array}{c}
\mu_X \\
\mu_Y
\end{array}
\right),
\left(
\begin{array}{cc}
\sigma^2_X & \rho \sigma_X\sigma_Y\\
\rho \sigma_Y\sigma_X & \sigma^2_Y
\end{array}
\right)
\right]
$$

## Bivariate Normal Distribution



## Bivariate Normal Distribution

```{=latex}
{\scriptsize
\begin{equation*}
N\left[
\left(
\begin{array}{c}
1 \\
-2
\end{array}
\right),
\left(
\begin{array}{cc}
2 & 0 \times \sqrt{2 \times 1.5}\\
0 \times \sqrt{2 \times 1.5} & 1.5
\end{array}
\right)
\right]
\end{equation*}
}
```

```{r}
library(MASS)
library(tidyverse)
library(mvtnorm)
x <- seq(-3, 5, length.out = 100)
y <- seq(-8, 3, length.out = 100)
grid <- expand.grid(x = x, y = y)

sigs <- matrix(c(2 , 0*sqrt(2*1.5), 0*sqrt(2*1.5), 1.5), nrow = 2)
mu <- c(1, -2)
# Compute density
z <- matrix(dmvnorm(grid, mean = mu, sigma = sigs),
            nrow = length(x))
persp(
  x,
  y,
  z,
  theta = 30,
  phi = 30,
  col = "lightblue",
  shade = 0.5,
  ticktype = "detailed",
  xlab = "X",
  ylab = "Y",
  zlab = "Density"
)

```




## Bivariate Normal Distribution

```{=latex}
{\scriptsize
\begin{equation*}
N\left[
\left(
\begin{array}{c}
1 \\
-2
\end{array}
\right),
\left(
\begin{array}{cc}
2 & 0 \times \sqrt{2 \times 1.5}\\
0 \times \sqrt{2 \times 1.5} & 1.5
\end{array}
\right)
\right]
\end{equation*}
}
```

```{r}
library(MASS)
library(tidyverse)

n <- 10000
data <- mvrnorm(n, mu = mu, Sigma = sigs)

df <- as.data.frame(data)
colnames(df) <- c("X", "Y")


ggplot(df, aes(x = X, y = Y)) +
  geom_point(alpha = 0.3) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = 0.4) +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Bivariate Normal Distribution",
       x = "X",
       y = "Y")


```

## Bivariate Normal Distribution

```{=latex}
{\scriptsize
\begin{equation*}
N\left[
\left(
\begin{array}{c}
1 \\
-2
\end{array}
\right),
\left(
\begin{array}{cc}
2 & 0.25 \times \sqrt{2 \times 1.5}\\
0.25 \times \sqrt{2 \times 1.5} & 1.5
\end{array}
\right)
\right]
\end{equation*}
}
```

```{r}
sigs <- matrix(c(2 , 0.25*sqrt(2*1.5), 0.25*sqrt(2*1.5), 1.5), nrow = 2)
n <- 10000
data <- mvrnorm(n, mu = mu, Sigma = sigs)

df <- as.data.frame(data)
colnames(df) <- c("X", "Y")


ggplot(df, aes(x = X, y = Y)) +
  geom_point(alpha = 0.3) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = 0.4) +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Bivariate Normal Distribution",
       x = "X",
       y = "Y")
```


## Bivariate Normal Distribution

```{=latex}
{\scriptsize
\begin{equation*}
N\left[
\left(
\begin{array}{c}
1 \\
-2
\end{array}
\right),
\left(
\begin{array}{cc}
2 & -0.55 \times \sqrt{2 \times 1.5}\\
-0.55 \times \sqrt{2 \times 1.5} & 1.5
\end{array}
\right)
\right]
\end{equation*}
}
```

```{r}

sigs <- matrix(c(2 , -0.55*sqrt(2*1.5), -0.55*sqrt(2*1.5), 1.5), nrow = 2)
n <- 10000
data <- mvrnorm(n, mu = mu, Sigma = sigs)

df <- as.data.frame(data)
colnames(df) <- c("X", "Y")


ggplot(df, aes(x = X, y = Y)) +
  geom_point(alpha = 0.3) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = 0.4) +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Bivariate Normal Distribution",
       x = "X",
       y = "Y")

```



# Marginal Density Function

## Marginal Density Functions

A Marginal Density Function is density function of one random variable from a random vector.

## Marginal Discrete Probability Mass Function

Let $X$ and $Y$ be 2 discrete random variables, with a joint distribution function of

$$
p_{X,Y}(x, y) = P(X = x, Y = y)
$$

The marginal distribution of $X$ is defined as

$$
p_{X}(x) = \sum_{y}p_{X,Y}(x,y)
$$

## Marginal Continuous Density Function

Let $X$ and $Y$ be 2 continuous random variables, with a joint density function of $f_{X,Y}(x,y)$. The marginal distribution of $X$ is defined as

$$
f_{X}(x) = \int_{y}f_{X,Y}(x,y)dy
$$

## Example

$$
f_{X,Y}(x,y) \left\{\begin{array}{cc}
2x & 0\le y \le 1;\ 0 \le x\le 1\\
0 & \mathrm{otherwise}
\end{array}\right.
$$

Find $f_X(x)$

## Bivariate Marginal Density


# Conditional Distributions

## Conditional Distributions

A conditional distribution provides the probability of a random variable, given that it was conditioned on the value of a second random variable.

## Discrete Conditional Distributions

Let $X$ and $Y$ be 2 discrete random variables, with a joint distribution function of

$$
p_{X,Y}(x, y) = P(X = x, Y = y)
$$

The conditional distribution of $X|Y=y$ is defined as

$$
p_{X|Y = y}(x) = \frac{p_{X,Y}(x,y)}{p_{Y}(y)}
$$

## Continuous Conditional Distributions

Let $X$ and $Y$ be 2 continuous random variables, with a joint density function of $f_{X,Y}(x,y)$. The conditional distribution of $X|Y=y$ is defined as

$$
f_{X|Y=y}(x)= \frac{f_{X,Y}(x,y)}{f_{Y}(y)}
$$

## Bivariae Normal Conditional Distribution


# Independence

## Independent Random Variables

Random variables are considered independent of each other if the probability of one variable does not affect the probability of another variable.

## Discrete Independent Random Variables

Let $X$ and $Y$ be 2 discrete random variables, with a joint density function of $p_{X,Y}(x,y)$. $X$ is independent of $Y$ if and only if

$$
p_{X,Y}(x,y) = p_{X}(x)p_{Y}(y)
$$

## Continuous Independent Random Variables

Let $X$ and $Y$ be 2 continuous random variables, with a joint density function of $f_{X,Y}(x,y)$. $X$ is independent of $Y$ if and only if

$$
f_{X,Y}(x,y) = f_{X}(x)f_{Y}(y)
$$

## Matrix Algebra

$$
A = \left(\begin{array}{cc}
a_1 & 0\\
0 & a_2
\end{array}\right)
$$

$$
\det(A) = a_1a_2
$$

$$
A^{-1}=\left(\begin{array}{cc}
1/a_1 & 0 \\
0 & 1/a_2
\end{array}\right)
$$

## Example

$$
\left(\begin{array}{c}
X\\
Y
\end{array}\right)\sim N \left\{
\left(\begin{array}{c}
\mu_x\\
\mu_y
\end{array}\right),\left(\begin{array}{cc}
\sigma_x^2 & 0\\
0 & \sigma_y^2
\end{array}\right)
\right\}
$$

Show that $X\perp Y$.

$$
f_{X,Y}(x,y)=\det(2\pi\Sigma)^{-1/2}\exp\left\{-\frac{1}{2}(\boldsymbol{w}-\boldsymbol\mu)^T\Sigma^{-1}(\boldsymbol w-\boldsymbol\mu)\right\}
$$

where $\Sigma=\left(\begin{array}{cc}\sigma_y^2 & 0\\0 & \sigma_y^2\end{array}\right)$, $\boldsymbol \mu = \left(\begin{array}{cc}\mu_x\\ \mu_y \end{array}\right)$, and $\boldsymbol w = \left(\begin{array}{cc} x\\ y \end{array}\right)$

# Covariance

## Covariance

Let $X$ and $Y$ be 2 random variables with mean $E(X)=\mu_x$ and $E(Y)=\mu_y$, respectively. The covariance of $X$ and $Y$ is defined as


$$
Cov(X,Y) = E\{(X-\mu_x)(Y-\mu_y)\}
$$

$$
Cov(X,Y) = E(XY)-\mu_x\mu_y
$$


## Covariance

If $X$ and $Y$ are independent random variables, then

$$
Cov(X,Y)=0
$$

## Correlation

The correlation of $X$ and $Y$ is defined as

$$
\rho = Cor(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
$$



# Expectations

## Expectations

Let $X_1, X_2, \ldots,X_n$ be a set of random variables, the expectation of a function $g(X_1,\ldots, X_n)$ is defined as

$$
E\{g(X_1,\ldots, X_n)\} = \sum_{x_1\in X_1}\cdots\sum_{x_n\in X_n}g(X_1,\ldots, X_n)p(x_1,\ldots,x_n)
$$

or

$$
E\{g(\boldsymbol X)\} = \int_{x_1\in X_1}\cdots\int_{x_n\in X_n}g(\boldsymbol X)f(\boldsymbol X)dx_n \cdots dx_1
$$

-   $\boldsymbol X = (X_1,\cdots, X_n)$

## Expected Value and Variance of Linear Functions

Let $X_1,\ldots,X_n$ and $Y_1,\ldots,Y_m$ be random variables with $E(X_i)=\mu_i$ and $E(Y_j)=\tau_j$. Furthermore, let $U = \sum^n_{i=1}a_iX_i$ and $V=\sum^m_{j=1}b_jY_j$ where $\{a_i\}^n_{i=1}$ and $\{b_j\}_{j=1}^m$ are constants. We have the following properties:

-   $E(U)=\sum_{i=1}^na_i\mu_i$

-   $Var(U)=\sum^n_{i=1}a_i^2Var(X_i)+2\underset{i<j}{\sum\sum}a_ia_jCov(X_i,X_j)$

-   $Cov(U,V)=\sum^n_{i=1}\sum^m_{j=1}Cov(X_i,Y_j)$

## Conditional Expectations

Let $X_1$ and $X_2$ be two random variables, the conditional expectation of $g(X_1)$, given $X_2=x_2$, is defined as

$$
E\{g(X_1)|X_2=x_2\}=\sum_{x_1}g(x_1)p(x_1|x_2)
$$

or

$$
E\{g(X_1)|X_2=x_2\}=\int_{x_1}g(x_1)f(x_1|x_2)dx_1.
$$

## Conditional Expectations

Furthermore,

$$
E(X_1)=E_{X_2}\{E_{X_1|X_2}(X_1|X_2)\}
$$

and

$$
Var(X_1) = E_{X_2}\{Var_{X_1|X_2}(X_1|X_2)\} + Var_{X_2}\{E_{X_1|X_2}(X_1|X_2)\}
$$

