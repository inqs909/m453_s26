{
  "hash": "f5781cc204a1f085cc2bf4c90cf1e578",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sampling Distrbutions\"\nformat:\n  beamer:\n    theme: gotham\n    colortheme: dolphin\n    fonttheme: professionalfonts\n    fontsize: 14pt\nheader-includes:\n  - \\setbeamertemplate{footline}{\n      \\hfill\n      \\usebeamerfont{page number in head/foot}\n      \\insertframenumber{} / \\inserttotalframenumber\n      \\kern1em\\vskip2pt\n    }\n  - \\gothamset{sectionframe default=off}\nknitr:\n  opts_chunk: \n    echo: true\n    eval: true\n    message: false\n    warnings: false\n    comment: \"#>\" \n\neditor: source\n---\n\n\n\n\n\n# Expectations\n\n## Expectations\n\nLet $\\boldsymbol X = (X_1, X_2, \\ldots,X_n)^\\mathrm{T}$ be a set of random variables, the expectation of a function $g(\\boldsymbol X)$ is defined as\n\n```{=latex}\n{\\small\n\\begin{equation*}\nE\\{g(\\boldsymbol X)\\} = \\sum_{x_1\\in X_1}\\cdots\\sum_{x_n\\in X_n}g(\\boldsymbol X)p(\\boldsymbol x, \\boldsymbol \\theta)\n\\end{equation*}\n}\n```\n\nor\n\n```{=latex}\n{\\small\n\\begin{equation*}\nE\\{g(\\boldsymbol X)\\} = \\int_{x_1\\in X_1}\\cdots\\int_{x_n\\in X_n}g(\\boldsymbol X)f(\\boldsymbol x, \\boldsymbol \\theta)dx_n \\cdots dx_1\n\\end{equation*}\n}\n```\n\n\n## Expected Value and Variance of Linear Functions\n\nLet $X_1,\\ldots,X_n$ and $Y_1,\\ldots,Y_m$ be random variables with $E(X_i)=\\mu_i$ and $E(Y_j)=\\tau_j$. Furthermore, let $U = \\sum^n_{i=1}a_iX_i$ and $V=\\sum^m_{j=1}b_jY_j$ where $\\{a_i\\}^n_{i=1}$ and $\\{b_j\\}_{j=1}^m$ are constants. We have the following properties:\n\n-   $E(U)=\\sum_{i=1}^na_i\\mu_i$\n\n-   $Var(U)=\\sum^n_{i=1}a_i^2Var(X_i)+2\\underset{i<j}{\\sum\\sum}a_ia_jCov(X_i,X_j)$\n\n-   $Cov(U,V)=\\sum^n_{i=1}\\sum^m_{j=1}Cov(X_i,Y_j)$\n\n## Conditional Expectations\n\nLet $X_1$ and $X_2$ be two random variables, the conditional expectation of $g(X_1)$, given $X_2=x_2$, is defined as\n\n$$\nE\\{g(X_1)|X_2=x_2\\}=\\sum_{x_1}g(x_1)p(x_1|x_2)\n$$\n\nor\n\n$$\nE\\{g(X_1)|X_2=x_2\\}=\\int_{x_1}g(x_1)f(x_1|x_2)dx_1.\n$$\n\n## Conditional Expectations\n\nFurthermore,\n\n```{=latex}\n{\\small\n\\begin{equation*}\nE(X_1)=E_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\end{equation*}\n}\n```\nand\n\n```{=latex}\n{\\footnotesize\n\\begin{equation*}\nVar(X_1) = E_{X_2}\\{Var_{X_1|X_2}(X_1|X_2)\\} + Var_{X_2}\\{E_{X_1|X_2}(X_1|X_2)\\}\n\\end{equation*}\n}\n```\n\n\n# Statistics\n\n## Sample\n\nWhen collecting data to construct a sample, the sample is a collection of random variables. Therefore, the sample can be subjected to probability properties.\n\n## iid Random Variables\n\nA sample of random variables are said to be iid if they are identical and independentally distributed.\n\nFor example, $X$ and $Y$ are iid, if $X$ and $Y$ has the same distribution $f(\\theta)$ and $X \\perp  Y$\n\n## Random Sample\n\n## Statistics\n\nA statistic is a transformation of the the sample data. Before data is calculated, a statistic from a sample can take any value. Therefore, a statistic must be a random variable.\n\n\n## Mean\n\nLet $X_1, X_2, \\ldots, X_n$ be a random sample that is *iid*.\nThe mean is defined as:\n\n$$\n\\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i\n$$\n\n## Variance\n\nLet $X_1, X_2, \\ldots, X_n$ be a random sample that is *iid*.\nThe variance is defined as:\n\n```{=latex}\n{\\small\n\\begin{equation*}\ns^2 = \\frac{1}{n-1}\\sum^n_{i=1} (X_i - \\bar X)^2 = \\frac{1}{n-1}\\sum^n_{i=1}X_i - n \\bar{X}^2\n\\end{equation*}\n}\n```\n\n\n\n# Sampling Distributions\n\n## Sampling Distributions\n\nA sampling distribution is the distribution of a statistic. Many known statistics have a known distribution.\n\n## $\\bar X$\n\nLet $X_1, X_2, \\ldots, X_n\\overset{iid}{\\sim}N(\\mu,\\sigma^2)$ , show that $\\bar X \\sim N(\\mu,\\sigma^2/n)$. \n\n## Sum of $\\chi^2_1$\n\nLet $Z_1^2, \\ldots, Z_n^2$ be a iid $\\chi^2_1$. Find $Y = \\sum^n_{i=1} Z_i^2$ \n\n## $s^2$\n\n## t-distribution\n\nLet $Z\\sim N(0,1)$, $W\\sim \\chi^2_\\nu$, $Z\\perp W$; therefore:\n\n$$\nT=\\frac{Z}{\\sqrt{W/\\nu}} \\sim t_\\nu\n$$\n\n## F-distribution\n\nLet $W_1\\sim\\chi^2_{\\nu_1}$ $W_2\\sim\\chi^2_{\\nu_2}$, and $W_1\\perp W_2$; therefore:\n\n$$\nF = \\frac{W_1/\\nu_1}{W_2/\\nu_2}\\sim F_{\\nu_1,\\nu_2}\n$$\n\n# Order Statistics\n\n##  Order Statistics\n\nOrder statistics are a fundamental concept in statistics and probability, dealing with the properties of sorted random variables. They provide insights into the distribution and behavior of sample data, such as minimum, maximum, and quantiles. Understanding order statistics is crucial in various fields such as risk management, quality control, and data analysis.\n\n## Order Statistics\n\n  Let $X_1, X_2, \\ldots, X_n$ be a sample of $n$ independent and identically distributed (i.i.d.) random variables with a common probability density function $f(x)$. The order statistics are the sorted values of this sample, denoted as:\n  \n$$\nX_{(1)} \\leq X_{(2)} \\leq \\cdots \\leq X_{(n)}\n$$\n  \n  Here, $X_{(1)}$ is the minimum, and $X_{(n)}$ is the maximum of the sample.\n\n## Order Statistics\n\n  - $X_{(k)}$: The $k$-th order statistic, representing the $k$-th smallest value in the sample.\n  - $X_{(1)}, X_{(n)}$: The minimum and maximum of the sample, respectively.\n\n## Distribution of Order Statistic\n\nThe distribution of the $k$-th order statistic $X_{(k)}$ can be derived using combinatorial arguments. Its PDF is given by:\n$$\nf_{X_{(k)}}(x) = \\frac{n!}{(k-1)!(n-k)!} [F(x)]^{k-1} [1 - F(x)]^{n-k} f(x)\n$$\n\n  This formula shows how the distribution of $X_{(k)}$ depends on the underlying distribution of the sample and its position $k$.\n\n\n\n# Central Limit Theorem\n\n## Central Limit Theorem\n\nLet $X_1, X_2, \\ldots, X_n$ be identical and independent distributed random variables with $E(X_i)=\\mu$ and $Var(X_i) = \\sigmaÂ²$. We define\n\n$$\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = x\\frac{1}{n}\\sum^n_{i=1}X_i.\n$$\n\nThen, the distribution of the function $Y_n$ converges to a standard normal distribution function as $n\\rightarrow \\infty$.\n\n## Central Limit Theorem\n\n$$\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n$$\n\n## Central Limit Theorem Proof\n\n\n## Example\n\nLet $X_1, \\ldots, X_n \\overset{iid}{\\sim} \\chi^2_p$, the MGF is $M(t)=(1-2t)^{-p/2}$. Find the distribution of $\\bar X$ as $n \\rightarrow \\infty$.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}